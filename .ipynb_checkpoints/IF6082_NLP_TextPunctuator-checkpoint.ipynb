{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZoEXY3170sOO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dWfbyf2Jz9QE"
   },
   "outputs": [],
   "source": [
    "dataFile = 'data/dummy-training-monolingual-europarl-en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "U8HlPydF0m9l",
    "outputId": "274bdaec-3539-401b-871d-84b7dbcc3f93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    0\n",
      "0                           Resumption of the session\n",
      "1   I declare resumed the session of the European ...\n",
      "2   Although, as you will have seen, the dreaded '...\n",
      "3   You have requested a debate on this subject in...\n",
      "4   In the meantime, I should like to observe a mi...\n",
      "5      Please rise, then, for this minute' s silence.\n",
      "6   (The House rose and observed a minute' s silence)\n",
      "7               Madam President, on a point of order.\n",
      "8   You will be aware from the press and televisio...\n",
      "9   One of the people assassinated very recently i...\n",
      "10  Would it be appropriate for you, Madam Preside...\n",
      "11  Yes, Mr Evans, I feel an initiative of the typ...\n",
      "12  If the House agrees, I shall do as Mr Evans ha...\n",
      "13              Madam President, on a point of order.\n",
      "14  I would like your advice about Rule 143 concer...\n",
      "15  My question relates to something that will com...\n",
      "16  The Cunha report on multiannual guidance progr...\n",
      "17  It says that this should be done despite the p...\n",
      "18  I believe that the principle of relative stabi...\n",
      "19  I want to know whether one can raise an object...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(dataFile, sep=\"\\n\", header=None)\n",
    "print(df)\n",
    "# print(df.loc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for data cleaning\n",
    "\n",
    "DOT_LIKE = ',;.!?'\n",
    "DOT_LIKE_AND_SPACE = ',;.!? '\n",
    "\n",
    "def cleanData(inputFile):\n",
    "    sys.stderr.write(\"Cleaning data \" + inputFile + \"\\n\")\n",
    "    mappings = OrderedDict([\n",
    "        (re.compile(\"['’]\"), \"'\"),\n",
    "        (re.compile(\"' s([\" + DOT_LIKE_AND_SPACE + \"])\"), \"'s\\g<1>\"), # Removes strange text mistake pattern in europarl data.\n",
    "        (re.compile(\"n't\"), \" n't\"),\n",
    "        (re.compile(\" '([^\" + DOT_LIKE + \"']*)'\"), '. \\g<1>.'), # Remove quoting apostrophes.\n",
    "        (re.compile(\"'([^t])\"), \" '\\g<1>\"), # Separate tokens like \"'s\" \"'ll\" and so on.\n",
    "        (re.compile('\\([^)]*\\)'), ''), # Removes bracketed.\n",
    "        (re.compile('[-—]'), ' '), # Dash to space.\n",
    "        (re.compile('[^a-z0-9A-Z\\',\\.?! ]'), ' '), # Other unknown to space.\n",
    "        (re.compile('^$|^\\.$'), ''), # Removes empty line.\n",
    "    ])\n",
    "    cleanFile = inputFile + '.clean'\n",
    "    regexProcess(mappings, inputFile, cleanFile)\n",
    "    return cleanFile\n",
    "\n",
    "def regexProcess(mappings, inputFile, outputFile):\n",
    "    with open(outputFile, 'w', encoding=\"utf8\") as output:\n",
    "        with open(inputFile, encoding=\"utf8\") as input:\n",
    "            for fullLine in input:\n",
    "                line = fullLine.rstrip()\n",
    "                for pattern, replacement in mappings.items():\n",
    "                    line = pattern.sub(replacement, line)\n",
    "                if len(line) == 0:\n",
    "                    continue\n",
    "                output.write(line + \" \")\n",
    "    return outputFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/dummy-training-monolingual-europarl-en.clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning data data/dummy-training-monolingual-europarl-en\n"
     ]
    }
   ],
   "source": [
    "cleanData(dataFile)\n",
    "cleaned_data = dataFile + \".clean\"\n",
    "print(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for data sampling\n",
    "\n",
    "def sampleData(\n",
    "        sampleCount,\n",
    "        inputFile,\n",
    "        weighted=True,\n",
    "        testPercentage=0.8):\n",
    "    import itertools\n",
    "    from random import randint\n",
    "\n",
    "    outputFile = inputFile + \".samples\"\n",
    "    sys.stderr.write(\"Sampling data \" + inputFile + ' into ' + outputFile + \"\\n\")\n",
    "    LOG_SAMPLE_NUM_STEP = 10000\n",
    "    DOT_LIKE_REGEX = re.compile('.*[' + DOT_LIKE + ']')\n",
    "\n",
    "    def incrementSampleNum(sampleNum):\n",
    "        sampleNum += 1\n",
    "        if sampleNum % LOG_SAMPLE_NUM_STEP == 0:\n",
    "            sys.stderr.write('sampleNum: ' + str(sampleNum) + \"\\n\")\n",
    "        return sampleNum\n",
    "\n",
    "    def readwords(mfile):\n",
    "        byte_stream = itertools.groupby(\n",
    "            itertools.takewhile(lambda c: bool(c),\n",
    "                                map(mfile.read,\n",
    "                                    itertools.repeat(1))), str.isspace)\n",
    "\n",
    "        return (\"\".join(group) for pred, group in byte_stream if not pred)\n",
    "\n",
    "    def samplingTestValues(sampleNum, sampleCount, testPercentage=0.8):\n",
    "        return int(sampleCount * testPercentage) < sampleNum\n",
    "\n",
    "    def write(output, window, label):\n",
    "        output.write(' '.join(window))\n",
    "        output.write(' ' + str(label))\n",
    "        output.write('\\n')\n",
    "\n",
    "    def skipNonDotSample(weighted):\n",
    "        DOT_WEIGHT = 1\n",
    "        \"\"\" Skip non dot samples to prevent local minima of no dots. \"\"\"\n",
    "        return weighted and randint(0, 9) < DOT_WEIGHT\n",
    "\n",
    "    def skip(weighted):\n",
    "        \"\"\" Skips for more diverse input. \"\"\"\n",
    "        return weighted and randint(0, 9) < 3\n",
    "\n",
    "    samples = []\n",
    "    labels = []\n",
    "    samplingTestValues = False\n",
    "    with open(outputFile, 'w', encoding=\"utf8\") as output:\n",
    "        with open(outputFile + \".test\", 'w', encoding=\"utf8\") as testOutput:\n",
    "            with open(inputFile, 'r', encoding=\"utf8\") as input:\n",
    "                window = []\n",
    "                sampleNum = 0\n",
    "                for word in readwords(input):\n",
    "                    if len(window) < WORDS_PER_SAMPLE_SIZE:\n",
    "                        window.append(word)\n",
    "                        continue\n",
    "                    if sampleNum != 0:\n",
    "                        window.append(word)\n",
    "                        window.pop(0)\n",
    "                    middle = window[-DETECTION_INDEX]\n",
    "                    if skip(weighted):\n",
    "                        continue\n",
    "                    if DOT_LIKE_REGEX.match(middle) is not None:\n",
    "                        label = True\n",
    "                    else:\n",
    "                        label = False\n",
    "                    if samplingTestValues:\n",
    "                        write(testOutput, window, label)\n",
    "                    else:\n",
    "                        samples.append(' '.join(window))\n",
    "                        labels.append(label)\n",
    "                        write(output, window, label)\n",
    "                    sampleNum = incrementSampleNum(sampleNum)\n",
    "                    if int(sampleCount * testPercentage) < sampleNum + 1:\n",
    "                        samplingTestValues = True\n",
    "                        weighted = False\n",
    "                    if 1 + sampleNum > sampleCount:\n",
    "                        break\n",
    "    return labels, samples\n",
    "\n",
    "def loadSamples(samplesCount, source):\n",
    "    sys.stderr.write('Loading maximum ' + str(samplesCount) + ' samples from ' + source + \"\\n\")\n",
    "    with open(source, 'r', encoding=\"utf8\") as input:\n",
    "        samples = []\n",
    "        labels = []\n",
    "        for fullLine in input:\n",
    "            line = fullLine.rstrip()\n",
    "            split = line.split(' ')\n",
    "            samples.append(' '.join(split[:-1]))\n",
    "            if split[-1] == \"True\":\n",
    "                labels.append(True)\n",
    "            else:\n",
    "                labels.append(False)\n",
    "            if len(samples) > samplesCount:\n",
    "                break\n",
    "        return labels, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"labels, samples = sampleData(5000000, dataFile + .clean, weighted=False)\")\n",
    "labels, samples = sampleData(5000000, dataFile + \".clean\", weighted=False)\n",
    "\n",
    "print(\"labels, samples = loadSamples(5000000, dataFile + .clean.samples)\")\n",
    "labels, samples = loadSamples(5000000, dataFile + \".clean.samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for Word Index\n",
    "\n",
    "def saveWordIndex(samples):\n",
    "    sys.stderr.write('Building word index.' + \"\\n\")\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(samples)\n",
    "    wordIndex = {}\n",
    "    for num, item in enumerate(tokenizer.word_index.items()):\n",
    "        if num >= MAX_NB_WORDS - 1:\n",
    "            break\n",
    "        else:\n",
    "            wordIndex[item[0]] = item[1]\n",
    "    saveObject(wordIndex, 'wordIndex')\n",
    "    sys.stderr.write('Found %s unique tokens.' % len(wordIndex) + \"\\n\")\n",
    "    return wordIndex\n",
    "\n",
    "def loadWordIndex():\n",
    "    return loadObject('wordIndex')\n",
    "\n",
    "def loadObject(name):\n",
    "    \"\"\" :rtype: dict \"\"\"\n",
    "    # return np.load(os.path.join(MODEL_DATA_DIR, name + '.npy')).item()\n",
    "\n",
    "    # save np.load\n",
    "    np_load_old = np.load\n",
    "    # modify the default parameters of np.load\n",
    "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "    # call load_data with allow_pickle implicitly set to true\n",
    "    load = np.load(os.path.join(MODEL_DATA_DIR, name + '.npy')).item()\n",
    "    # restore np.load for future normal usage\n",
    "    np.load = np_load_old\n",
    "\n",
    "    return load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for tokenization\n",
    "\n",
    "def tokenize(labels, samples, wordIndex):\n",
    "    sys.stderr.write('Tokenizing samples.' + \"\\n\")\n",
    "\n",
    "    tokenizedSamples = texts_to_sequences(wordIndex, samples, MAX_NB_WORDS)\n",
    "    paddedSamples = pad_sequences(tokenizedSamples, maxlen=WORDS_PER_SAMPLE_SIZE)\n",
    "\n",
    "    tokenizedLabels = to_categorical(np.asarray(labels))\n",
    "\n",
    "    sys.stderr.write('Shape of paddedSamples tensor:' + str(paddedSamples.shape) + \"\\n\")\n",
    "    sys.stderr.write('Shape of tokenizedLabels tensor:' + str(tokenizedLabels.shape) + \"\\n\")\n",
    "\n",
    "    return tokenizedLabels, paddedSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for splitting the data into a training set and a validation set\n",
    "\n",
    "def splitTrainingAndValidation(labels, samples):\n",
    "    indices = np.arange(samples.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    samples = samples[indices]\n",
    "    labels = labels[indices]\n",
    "    nb_validation_samples = int(VALIDATION_SPLIT * samples.shape[0])\n",
    "\n",
    "    xTrain = samples[:-nb_validation_samples]\n",
    "    yTrain = labels[:-nb_validation_samples]\n",
    "    xVal = samples[-nb_validation_samples:]\n",
    "    yVal = labels[-nb_validation_samples:]\n",
    "    return xTrain, yTrain, xVal, yVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for creating model with or without wordIndex\n",
    "\n",
    "def createModel(wordIndex=None):\n",
    "    sys.stderr.write('Creating model.' + \"\\n\")\n",
    "    model = Sequential()\n",
    "    model.add(createEmbeddingLayer(wordIndex))\n",
    "    model.add(Conv1D(512, 3, activation='relu'))\n",
    "    if wordIndex is not None:\n",
    "        model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(LABELS_COUNT, activation='softmax'))\n",
    "    # alternative optimizer: rmsprop, adam\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc', precision, recall, fbeta_score])\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "IF6082-NLP-TextPunctuator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
