{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is made for demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krLGG6e2AByu"
   },
   "source": [
    "### Use pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UTDByqhp5sW2"
   },
   "outputs": [],
   "source": [
    "# load file for Google Colab\n",
    "\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# model_file = \"Demo-Europarl-EN.pcl\"\n",
    "# pretrainedModelID = \"1VMnEe5eEzOwwAIkC9N9mAktBsJBlK_Ih\"\n",
    "\n",
    "# auth.authenticate_user()\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive = GoogleDrive(gauth)\n",
    "\n",
    "# downloaded = drive.CreateFile({'id': pretrainedModelID})\n",
    "# downloaded.GetContentFile(model_file)\n",
    "\n",
    "\n",
    "# load file for local Jupyter Noteboook\n",
    "\n",
    "model_file = \"pretrained-model/Demo-Europarl-EN.pcl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NN-6xImTDJRp"
   },
   "source": [
    "### models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RiuDvexyBJXu"
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "try:\n",
    "    import cPickle\n",
    "    cpickle_options = {}\n",
    "except ImportError:\n",
    "    import _pickle as cPickle\n",
    "    cpickle_options = { 'encoding': 'latin-1' }\n",
    "import os\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "def PReLU(a, x):\n",
    "    return T.maximum(0.0, x) + a * T.minimum(0.0, x)\n",
    "\n",
    "def ReLU(x):\n",
    "    return T.maximum(0.0, x)\n",
    "\n",
    "def _get_shape(i, o, keepdims):\n",
    "    if (i == 1 or o == 1) and not keepdims:\n",
    "        return (max(i,o),)\n",
    "    else:\n",
    "        return (i, o)\n",
    "\n",
    "def _slice(tensor, size, i):\n",
    "    \"\"\"Gets slice of columns of the tensor\"\"\"\n",
    "    if tensor.ndim == 2:\n",
    "        return tensor[:, i*size:(i+1)*size]\n",
    "    elif tensor.ndim == 1:\n",
    "        return tensor[i*size:(i+1)*size]\n",
    "    else:\n",
    "        raise NotImplementedError(\"Tensor should be 1 or 2 dimensional\")\n",
    "\n",
    "def weights_const(i, o, name, const, keepdims=False):\n",
    "    W_values = np.ones(_get_shape(i, o, keepdims)).astype(theano.config.floatX) * const\n",
    "    return theano.shared(value=W_values, name=name, borrow=True)\n",
    "\n",
    "def weights_identity(i, o, name, const, keepdims=False):\n",
    "    #\"A Simple Way to Initialize Recurrent Networks of Rectified Linear Units\" (2015) (http://arxiv.org/abs/1504.00941)\n",
    "    W_values = np.eye(*_get_shape(i, o, keepdims)).astype(theano.config.floatX) * const\n",
    "    return theano.shared(value=W_values, name=name, borrow=True)\n",
    "\n",
    "def weights_Glorot(i, o, name, rng, is_logistic_sigmoid=False, keepdims=False):\n",
    "    #http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n",
    "    d = np.sqrt(6. / (i + o))\n",
    "    if is_logistic_sigmoid:\n",
    "        d *= 4.\n",
    "    W_values = rng.uniform(low=-d, high=d, size=_get_shape(i, o, keepdims)).astype(theano.config.floatX)\n",
    "    return theano.shared(value=W_values, name=name, borrow=True)\n",
    "\n",
    "def load(file_path, minibatch_size, x, p=None):\n",
    "    try:\n",
    "        import cPickle\n",
    "    except ImportError:\n",
    "        import _pickle as cPickle\n",
    "    import theano\n",
    "    import numpy as np\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "        state = cPickle.load(f, **cpickle_options)\n",
    "\n",
    "    rng = np.random\n",
    "    rng.set_state(state[\"random_state\"])\n",
    "\n",
    "    net = GRU(\n",
    "        rng=rng,\n",
    "        x=x,\n",
    "        minibatch_size=minibatch_size,\n",
    "        n_hidden=state[\"n_hidden\"],\n",
    "        x_vocabulary=state[\"x_vocabulary\"],\n",
    "        y_vocabulary=state[\"y_vocabulary\"],\n",
    "        stage1_model_file_name=state.get(\"stage1_model_file_name\", None),\n",
    "        p=p\n",
    "        )\n",
    "\n",
    "    for net_param, state_param in zip(net.params, state[\"params\"]):\n",
    "        net_param.set_value(state_param, borrow=True)\n",
    "\n",
    "    gsums = [theano.shared(gsum) for gsum in state[\"gsums\"]] if state[\"gsums\"] else None\n",
    "\n",
    "    return net, (gsums, state[\"learning_rate\"], state[\"validation_ppl_history\"], state[\"epoch\"], rng)\n",
    "\n",
    "class GRULayer(object):\n",
    "\n",
    "    def __init__(self, rng, n_in, n_out, minibatch_size):\n",
    "        super(GRULayer, self).__init__()\n",
    "        # Notation from: An Empirical Exploration of Recurrent Network Architectures\n",
    "\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "\n",
    "        # Initial hidden state\n",
    "        self.h0 = theano.shared(value=np.zeros((minibatch_size, n_out)).astype(theano.config.floatX), name='h0', borrow=True)\n",
    "\n",
    "        # Gate parameters:\n",
    "        self.W_x = weights_Glorot(n_in, n_out*2, 'W_x', rng)\n",
    "        self.W_h = weights_Glorot(n_out, n_out*2, 'W_h', rng)\n",
    "        self.b = weights_const(1, n_out*2, 'b', 0)\n",
    "        # Input parameters\n",
    "        self.W_x_h = weights_Glorot(n_in, n_out, 'W_x_h', rng)\n",
    "        self.W_h_h = weights_Glorot(n_out, n_out, 'W_h_h', rng)\n",
    "        self.b_h = weights_const(1, n_out, 'b_h', 0)\n",
    "\n",
    "        self.params = [self.W_x, self.W_h, self.b, self.W_x_h, self.W_h_h, self.b_h]\n",
    "\n",
    "    def step(self, x_t, h_tm1):\n",
    "\n",
    "        rz = T.nnet.sigmoid(T.dot(x_t, self.W_x) + T.dot(h_tm1, self.W_h) + self.b)\n",
    "        r = _slice(rz, self.n_out, 0)\n",
    "        z = _slice(rz, self.n_out, 1)\n",
    "\n",
    "        h = T.tanh(T.dot(x_t, self.W_x_h) + T.dot(h_tm1 * r, self.W_h_h) + self.b_h)\n",
    "\n",
    "        h_t = z * h_tm1 + (1. - z) * h\n",
    "\n",
    "        return h_t\n",
    "\n",
    "class GRU(object):\n",
    "\n",
    "    def __init__(self, rng, x, minibatch_size, n_hidden, x_vocabulary, y_vocabulary, stage1_model_file_name=None, p=None):\n",
    "\n",
    "        assert not stage1_model_file_name and not p, \"Stage 1 model can't have stage 1 model\"\n",
    "\n",
    "        x_vocabulary_size = len(x_vocabulary)\n",
    "        y_vocabulary_size = len(y_vocabulary)\n",
    "\n",
    "        self.n_hidden = n_hidden\n",
    "        self.x_vocabulary = x_vocabulary\n",
    "        self.y_vocabulary = y_vocabulary\n",
    "\n",
    "        # input model\n",
    "        pretrained_embs_path = \"We.pcl\"\n",
    "        if os.path.exists(pretrained_embs_path):\n",
    "            print(\"Found pretrained embeddings in '%s'. Using them...\" % pretrained_embs_path)\n",
    "            with open(pretrained_embs_path, 'rb') as f:\n",
    "                We = cPickle.load(f, **cpickle_options)\n",
    "            n_emb = len(We[0])\n",
    "            We.append([0.1]*n_emb) # END\n",
    "            We.append([0.0]*n_emb) # UNK - both quite arbitrary initializations\n",
    "\n",
    "            We = np.array(We).astype(theano.config.floatX)\n",
    "            self.We = theano.shared(value=We, name=\"We\", borrow=True)\n",
    "        else:\n",
    "            n_emb = n_hidden\n",
    "            self.We = weights_Glorot(x_vocabulary_size, n_emb, 'We', rng) # Share embeddings between forward and backward model\n",
    "\n",
    "        self.GRU_f = GRULayer(rng=rng, n_in=n_emb, n_out=n_hidden, minibatch_size=minibatch_size)\n",
    "        self.GRU_b = GRULayer(rng=rng, n_in=n_emb, n_out=n_hidden, minibatch_size=minibatch_size)\n",
    "\n",
    "        # output model\n",
    "        self.GRU = GRULayer(rng=rng, n_in=n_hidden*2, n_out=n_hidden, minibatch_size=minibatch_size)\n",
    "        self.Wy = weights_const(n_hidden, y_vocabulary_size, 'Wy', 0)\n",
    "        self.by = weights_const(1, y_vocabulary_size, 'by', 0)\n",
    "\n",
    "        # attention model\n",
    "        n_attention = n_hidden * 2 # to match concatenated forward and reverse model states\n",
    "        self.Wa_h = weights_Glorot(n_hidden, n_attention, 'Wa_h', rng) # output model previous hidden state to attention model weights\n",
    "        self.Wa_c = weights_Glorot(n_attention, n_attention, 'Wa_c', rng) # contexts to attention model weights\n",
    "        self.ba = weights_const(1, n_attention, 'ba', 0)\n",
    "        self.Wa_y = weights_Glorot(n_attention, 1, 'Wa_y', rng) # gives weights to contexts\n",
    "\n",
    "        # Late fusion parameters\n",
    "        self.Wf_h = weights_const(n_hidden, n_hidden, 'Wf_h', 0)\n",
    "        self.Wf_c = weights_const(n_attention, n_hidden, 'Wf_c', 0)\n",
    "        self.Wf_f = weights_const(n_hidden, n_hidden, 'Wf_f', 0)\n",
    "        self.bf = weights_const(1, n_hidden, 'by', 0)\n",
    "\n",
    "        self.params = [self.We,\n",
    "                       self.Wy, self.by,\n",
    "                       self.Wa_h, self.Wa_c, self.ba, self.Wa_y,\n",
    "                       self.Wf_h, self.Wf_c, self.Wf_f, self.bf]\n",
    "\n",
    "        self.params += self.GRU.params + self.GRU_f.params + self.GRU_b.params\n",
    "\n",
    "        # bi-directional recurrence\n",
    "        def input_recurrence(x_f_t, x_b_t, h_f_tm1, h_b_tm1):\n",
    "            h_f_t = self.GRU_f.step(x_t=x_f_t, h_tm1=h_f_tm1)\n",
    "            h_b_t = self.GRU_b.step(x_t=x_b_t, h_tm1=h_b_tm1)\n",
    "            return [h_f_t, h_b_t]\n",
    "\n",
    "        def output_recurrence(x_t, h_tm1, Wa_h, Wa_y, Wf_h, Wf_c, Wf_f, bf, Wy, by, context, projected_context):\n",
    "\n",
    "            # Attention model\n",
    "            h_a = T.tanh(projected_context + T.dot(h_tm1, Wa_h))\n",
    "            alphas = T.exp(T.dot(h_a, Wa_y))\n",
    "            alphas = alphas.reshape((alphas.shape[0], alphas.shape[1])) # drop 2-axis (sized 1)\n",
    "            alphas = alphas / alphas.sum(axis=0, keepdims=True)\n",
    "            weighted_context = (context * alphas[:,:,None]).sum(axis=0)\n",
    "\n",
    "            h_t = self.GRU.step(x_t=x_t, h_tm1=h_tm1)\n",
    "\n",
    "            # Late fusion\n",
    "            lfc = T.dot(weighted_context, Wf_c) # late fused context\n",
    "            fw = T.nnet.sigmoid(T.dot(lfc, Wf_f) + T.dot(h_t, Wf_h) + bf) # fusion weights\n",
    "            hf_t = lfc * fw + h_t # weighted fused context + hidden state\n",
    "\n",
    "            z = T.dot(hf_t, Wy) + by\n",
    "            y_t = T.nnet.softmax(z)\n",
    "\n",
    "            return [h_t, hf_t, y_t, alphas]\n",
    "\n",
    "        x_emb = self.We[x.flatten()].reshape((x.shape[0], minibatch_size, n_emb))\n",
    "\n",
    "        [h_f_t, h_b_t], _ = theano.scan(fn=input_recurrence,\n",
    "            sequences=[x_emb, x_emb[::-1]], # forward and backward sequences\n",
    "            outputs_info=[self.GRU_f.h0, self.GRU_b.h0])\n",
    "\n",
    "        # 0-axis is time steps, 1-axis is batch size and 2-axis is hidden layer size\n",
    "        context = T.concatenate([h_f_t, h_b_t[::-1]], axis=2)\n",
    "        projected_context = T.dot(context, self.Wa_c) + self.ba\n",
    "\n",
    "        [_, self.last_hidden_states, self.y, self.alphas], _ = theano.scan(fn=output_recurrence,\n",
    "            sequences=[context[1:]], # ignore the 1st word in context, because there's no punctuation before that\n",
    "            non_sequences=[self.Wa_h, self.Wa_y, self.Wf_h, self.Wf_c, self.Wf_f, self.bf, self.Wy, self.by, context, projected_context],\n",
    "            outputs_info=[self.GRU.h0, None, None, None])\n",
    "\n",
    "        print(\"Number of parameters is %d\" % sum(np.prod(p.shape.eval()) for p in self.params))\n",
    "\n",
    "        self.L1 = sum(abs(p).sum() for p in self.params)\n",
    "        self.L2_sqr = sum((p**2).sum() for p in self.params)\n",
    "\n",
    "    def cost(self, y):\n",
    "        num_outputs = self.y.shape[0]*self.y.shape[1] # time steps * number of parallel sequences in batch\n",
    "        output = self.y.reshape((num_outputs, self.y.shape[2]))\n",
    "        return -T.sum(T.log(output[T.arange(num_outputs), y.flatten()]))\n",
    "\n",
    "    def save(self, file_path, gsums=None, learning_rate=None, validation_ppl_history=None, best_validation_ppl=None, epoch=None, random_state=None):\n",
    "        try:\n",
    "            import cPickle\n",
    "        except ImportError:\n",
    "            import _pickle as cPickle\n",
    "        state = {\n",
    "            \"type\":                     self.__class__.__name__,\n",
    "            \"n_hidden\":                 self.n_hidden,\n",
    "            \"x_vocabulary\":             self.x_vocabulary,\n",
    "            \"y_vocabulary\":             self.y_vocabulary,\n",
    "            \"stage1_model_file_name\":   self.stage1_model_file_name if hasattr(self, \"stage1_model_file_name\") else None,\n",
    "            \"params\":                   [p.get_value(borrow=True) for p in self.params],\n",
    "            \"gsums\":                    [s.get_value(borrow=True) for s in gsums] if gsums else None,\n",
    "            \"learning_rate\":            learning_rate,\n",
    "            \"validation_ppl_history\":   validation_ppl_history,\n",
    "            \"epoch\":                    epoch,\n",
    "            \"random_state\":             random_state\n",
    "        }\n",
    "\n",
    "        with open(file_path, 'wb') as f:\n",
    "            cPickle.dump(state, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "class GRUstage2(GRU):\n",
    "\n",
    "    def __init__(self, rng, x, minibatch_size, n_hidden, x_vocabulary, y_vocabulary, stage1_model_file_name, p=None):\n",
    "\n",
    "        y_vocabulary_size = len(y_vocabulary)\n",
    "\n",
    "        self.stage1_model_file_name = stage1_model_file_name\n",
    "        self.stage1, _ = load(stage1_model_file_name, minibatch_size, x)\n",
    "\n",
    "        self.n_hidden = n_hidden\n",
    "        self.x_vocabulary = x_vocabulary\n",
    "        self.y_vocabulary = y_vocabulary\n",
    "\n",
    "        # output model\n",
    "        self.GRU = GRULayer(rng=rng, n_in=self.stage1.n_hidden + 1, n_out=n_hidden, minibatch_size=minibatch_size)\n",
    "        self.Wy = weights_const(n_hidden, y_vocabulary_size, 'Wy', 0)\n",
    "        self.by = weights_const(1, y_vocabulary_size, 'by', 0)\n",
    "\n",
    "        self.params = [self.Wy, self.by]\n",
    "        self.params += self.GRU.params\n",
    "\n",
    "        def recurrence(x_t, p_t, h_tm1, Wy, by):\n",
    "\n",
    "            h_t = self.GRU.step(x_t=T.concatenate((x_t, p_t.dimshuffle((0, 'x'))), axis=1), h_tm1=h_tm1)\n",
    "\n",
    "            z = T.dot(h_t, Wy) + by\n",
    "            y_t = T.nnet.softmax(z)\n",
    "\n",
    "            return [h_t, y_t]\n",
    "\n",
    "        [_, self.y], _ = theano.scan(fn=recurrence,\n",
    "            sequences=[self.stage1.last_hidden_states, p],\n",
    "            non_sequences=[self.Wy, self.by],\n",
    "            outputs_info=[self.GRU.h0, None])\n",
    "\n",
    "        print(\"Number of parameters is %d\" % sum(np.prod(p.shape.eval()) for p in self.params))\n",
    "        print(\"Number of parameters with stage1 params is %d\" % sum(np.prod(p.shape.eval()) for p in self.params + self.stage1.params))\n",
    "\n",
    "        self.L1 = sum(abs(p).sum() for p in self.params)\n",
    "        self.L2_sqr = sum((p**2).sum() for p in self.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zWzskMCVD1ah"
   },
   "source": [
    "### demo_play_with_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5UR4DfCE3ZY"
   },
   "outputs": [],
   "source": [
    "# constant from data.py\n",
    "\n",
    "END = \"</S>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "SPACE = \"_SPACE\"\n",
    "MAX_SEQUENCE_LEN = 50\n",
    "EOS_TOKENS = {\".PERIOD\", \"?QUESTIONMARK\", \"!EXCLAMATIONMARK\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tlz6JejPD8fp"
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import sys\n",
    "import re\n",
    "from io import open\n",
    "\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "def to_array(arr, dtype=np.int32):\n",
    "    # minibatch of 1 sequence as column\n",
    "    return np.array([arr], dtype=dtype).T\n",
    "\n",
    "def convert_punctuation_to_readable(punct_token):\n",
    "    if punct_token == SPACE:\n",
    "        return \" \"\n",
    "    else:\n",
    "        return punct_token[0]\n",
    "\n",
    "def punctuate(predict, word_vocabulary, punctuation_vocabulary, reverse_punctuation_vocabulary, reverse_word_vocabulary, text, f_out, show_unk):\n",
    "\n",
    "    if len(text) == 0:\n",
    "        sys.exit(\"Input text from stdin missing.\")\n",
    "\n",
    "    text = [w for w in text.split() if w not in punctuation_vocabulary] + [END]\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while True:\n",
    "\n",
    "        subsequence = text[i:i+MAX_SEQUENCE_LEN]\n",
    "\n",
    "        if len(subsequence) == 0:\n",
    "            break\n",
    "\n",
    "        converted_subsequence = [word_vocabulary.get(w, word_vocabulary[UNK]) for w in subsequence]\n",
    "        if show_unk:\n",
    "            subsequence = [reverse_word_vocabulary[w] for w in converted_subsequence]\n",
    "\n",
    "        y = predict(to_array(converted_subsequence))\n",
    "\n",
    "        f_out = f_out+subsequence[0]\n",
    "\n",
    "        last_eos_idx = 0\n",
    "        punctuations = []\n",
    "        for y_t in y:\n",
    "\n",
    "            p_i = np.argmax(y_t.flatten())\n",
    "            punctuation = reverse_punctuation_vocabulary[p_i]\n",
    "\n",
    "            punctuations.append(punctuation)\n",
    "\n",
    "            if punctuation in EOS_TOKENS:\n",
    "                last_eos_idx = len(punctuations) # we intentionally want the index of next element\n",
    "\n",
    "        if subsequence[-1] == END:\n",
    "            step = len(subsequence) - 1\n",
    "        elif last_eos_idx != 0:\n",
    "            step = last_eos_idx\n",
    "        else:\n",
    "            step = len(subsequence) - 1\n",
    "\n",
    "        for j in range(step):\n",
    "            f_out = f_out+(\" \" + punctuations[j] + \" \" if punctuations[j] != SPACE else \" \")\n",
    "            if j < step - 1:\n",
    "                f_out = f_out+(subsequence[1+j])\n",
    "\n",
    "        if subsequence[-1] == END:\n",
    "            break\n",
    "\n",
    "        i += step\n",
    "        \n",
    "    # convert punctuation tokens to actual punctuation\n",
    "    f_out = re.sub(r\"\\s,COMMA\", \",\", f_out)\n",
    "    f_out = re.sub(r\"\\s.PERIOD\", \".\", f_out)\n",
    "    f_out = re.sub(r\"\\s\\?QUESTIONMARK\", \"?\", f_out)\n",
    "    f_out = re.sub(r\"\\s!EXCLAMATIONMARK\", \"!\", f_out)\n",
    "    f_out = re.sub(r\"\\s-DASH\\s\", \"-\", f_out)\n",
    "    f_out = re.sub(r\"\\s:COLON\", \":\", f_out)\n",
    "    f_out = re.sub(r\"\\s;SEMICOLON\", \";\", f_out)\n",
    "\n",
    "    # add period as end of sentence punctuation, if there isn't any\n",
    "    last_char = f_out[-2]\n",
    "\n",
    "    if (last_char not in ['.', '?', '!']):\n",
    "        f_out = re.sub(r\"\\s$\", \".\", f_out)\n",
    "    \n",
    "    return f_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T9bYJU49DgF2"
   },
   "source": [
    "### Punctuate input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "tJuXmWp-_ndY",
    "outputId": "1616ef6d-3223-4dc0-d86c-2148443e3c06"
   },
   "outputs": [],
   "source": [
    "# show unknown or OOV words\n",
    "show_unk = False\n",
    "\n",
    "x = T.imatrix('x')\n",
    "\n",
    "print(\"Loading model parameters...\")\n",
    "net, _ = load(model_file, 1, x)\n",
    "\n",
    "print(\"Building model...\")\n",
    "predict = theano.function(inputs=[x], outputs=net.y)\n",
    "word_vocabulary = net.x_vocabulary\n",
    "punctuation_vocabulary = net.y_vocabulary\n",
    "reverse_word_vocabulary = {v:k for k,v in net.x_vocabulary.items()}\n",
    "reverse_punctuation_vocabulary = {v:k for k,v in net.y_vocabulary.items()}\n",
    "\n",
    "f_out = \"\"\n",
    "\n",
    "try:\n",
    "    text = raw_input(\"\\nTEXT: \").decode('utf-8')\n",
    "except NameError:\n",
    "    text = input(\"\\nTEXT: \")\n",
    "\n",
    "res = punctuate(predict, word_vocabulary, punctuation_vocabulary, reverse_punctuation_vocabulary, reverse_word_vocabulary, text, f_out, show_unk)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Akhz35Nw_Hqz"
   },
   "source": [
    "## Process file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "\n",
    "# show unknown or OOV words\n",
    "show_unk = False\n",
    "\n",
    "x = T.imatrix('x')\n",
    "\n",
    "print(\"Loading model parameters...\")\n",
    "net, _ = load(model_file, 1, x)\n",
    "\n",
    "print(\"Building model...\")\n",
    "predict = theano.function(inputs=[x], outputs=net.y)\n",
    "word_vocabulary = net.x_vocabulary\n",
    "punctuation_vocabulary = net.y_vocabulary\n",
    "reverse_word_vocabulary = {v:k for k,v in net.x_vocabulary.items()}\n",
    "reverse_punctuation_vocabulary = {v:k for k,v in net.y_vocabulary.items()}\n",
    "\n",
    "f_out = \"\"\n",
    "\n",
    "input_filename = \"in.txt\"\n",
    "output_filename = \"out.txt\"\n",
    "\n",
    "with open(output_filename, 'w', encoding='utf-8') as out_txt:\n",
    "    with open(input_filename, 'r', encoding='utf-8') as text:\n",
    "\n",
    "        for line in text:\n",
    "            \n",
    "            res = punctuate(predict, word_vocabulary, punctuation_vocabulary, reverse_punctuation_vocabulary, reverse_word_vocabulary, line, f_out, show_unk)\n",
    "\n",
    "            out_txt.write(res)\n",
    "\n",
    "\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_filename = \"in.txt\"\n",
    "output_filename = \"out.txt\"\n",
    "\n",
    "with open(output_filename, 'w', encoding='utf-8') as out_txt:\n",
    "    with open(input_filename, 'r', encoding='utf-8') as text:\n",
    "\n",
    "        for line in text:\n",
    "\n",
    "#             line = process_line(line)\n",
    "\n",
    "            out_txt.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "krLGG6e2AByu",
    "NN-6xImTDJRp"
   ],
   "name": "Punctuator2_IF6082_NLP_TextPunctuator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
